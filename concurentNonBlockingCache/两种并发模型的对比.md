这个例子中为了使用并发对网络请求进行优化, 使用了**共享变量**和**顺序通讯**两种模型实现了重复抑制、非阻塞缓存的并发模型.

**共享变量:**
在Get4方法中, main函数为每个url创建一个goroutine, 然后这些goroutine共享cache. 这要求对cache的读和写一定要是同步的
1. 对写的同步比较好理解, 一个goroutine在写一个cache key时, 不能允许其它goroutine也同时在写这个key, 否则就是竞态
2. 对读的同比是出于两点
    1. 一个goroutine在读一个cache key的时候, 如果不同步, 此时可能会有另一个goroutine在写, 所以这个goroutine可能会读到不完整的结果, 只有另一个goroutine完成写入, 这个goroutine才能读到完整的结果
    2. 由于现代计算机的多cpu和cpu本地缓存设计, 如果不显式同步, 一个cpu内的goroutine执行结果不能确定被刷到内存, 这样对于其它cpu内的goroutine, 这个执行结果是不可见的

对共享变量的读写同步就避免的数据竞态问题, Get4中同步的写是写了一个key,并未这个key创建了一个通道后就释放了锁, 而不是等待耗时操作的结果才去释放, 相同的key再次来写时是等待这个key的通道ready, 实现重复抑制

**顺序通讯:**
这个模型的核型就是Server方法, 这个Server方法:
1. 监听memo对象的requests通道
2. 读写cache(或者说守卫cache)

执行流程
1. main函数创建一个memo后, 就立即启动了Server(), 
2. 再为每个url创建一个goroutine, 每个url goroutine向memo的requests通道发送请求消息(包含请求的key和请求自带的response通道), 发送后就立即监听这个请求的response通道
3. memo的Server方法收到请求消息后, 检查cache, 由于cache是Server内部的变量, 而且也仅有Server读写cache所以自然不会有任何并发问题
4. cache不存这个key就建一个entry, 写到对应的key下
5. 然后为这个请求创建两个goroutine: 
    1. 一个是发送请求的call goroutine, 请求结果放在entry的res, 然后关闭entry的ready通道
    2. 另一个是监听entry的ready通道, 一旦ok, 就把entry的res发送给请求的response通道
   这两个goroutine保证了Server的主goroutine是非阻塞的.
6. main函数创建的url goroutine接收到自己刚刚发送的请求的response通道的响应后结束

**总结**
1. 共享变量模型中虽然多个goroutine都可以读写, 但使用锁来保证任何时刻只会有一个在读写, 顺序通讯模型中使用一个goroutine专门负责读写, 让共享的变量限定在一个goroutine内
2. 防止耗时操作延长写对锁的占用时间, 先写入一个key, 然后使用监听通道的方式来等待结果准备完成.




